import os, faiss, json
from sentence_transformers import SentenceTransformer
import google.generativeai as genai
from fastmcp import FastMCP, Context
from contextlib import asynccontextmanager
from dotenv import load_dotenv

load_dotenv('./gemini.env', override=True)
GEMINI_API_KEY = os.getenv('GOOGLE_API_KEY')
if not GEMINI_API_KEY: raise EnvironmentError("...")

FAISS_INDEX_PATH = "./faiss.index"
DOCS_JSON_PATH = "./semantic_chunks.json"
EMBEDDING_MODEL_NAME = "all-MiniLM-L6-v2"
TOP_K = 5

class RAGContext:
    def __init__(self):
        self.index = None
        self.documents = None
        self.embedder = None
        self.gemini_model = None

    async def setup(self):
        self.index = faiss.read_index(FAISS_INDEX_PATH)
        with open(DOCS_JSON_PATH, 'r') as f:
            chunk_dicts = json.load(f)
        self.documents = [d["chunk"] for d in chunk_dicts]
        self.embedder = SentenceTransformer(EMBEDDING_MODEL_NAME)
        genai.configure(api_key=GEMINI_API_KEY)
        self.gemini_model = genai.GenerativeModel("gemini-1.5-flash")

    async def cleanup(self):
        # Clear out resources
        self.index = None
        self.documents = None
        self.embedder = None
        self.gemini_model = None

@asynccontextmanager
async def app_lifespan(server: FastMCP):
    rag = RAGContext()
    await rag.setup()
    try:
        yield rag
    finally:
        await rag.cleanup()

mcp = FastMCP("RAGServer", lifespan=app_lifespan)

# --- Define the RAG Query Tool ---
@mcp.tool()
async def rag_query(ctx: Context, user_query: str) -> str:
    """Performs a Retrieval-Augmented Generation (RAG) query using a FAISS index and Gemini.

    Args:
        ctx: The MCP context object, providing access to application state (our RAGContext).
        user_query: The question from the user.

    Returns:
        The answer generated by Gemini, augmented with retrieved context.
    """
    rag_context: RAGContext = ctx.app_data

    if not rag_context.index or not rag_context.documents or not rag_context.embedder or not rag_context.gemini_model:
        return "RAG system not initialized. Please check server logs for errors."

    print(f"[RAGServer Tool] Received query: '{user_query}'")

    # --- Embed Query and Normalize ---
    query_embedding = rag_context.embedder.encode([user_query], normalize_embeddings=True)

    # --- Perform FAISS Search ---
    D, I = rag_context.index.search(query_embedding, TOP_K)
    matched_docs = [rag_context.documents[i] for i in I[0] if i != -1] # Filter out -1 for missing docs

    if not matched_docs:
        print("[RAGServer Tool] No relevant documents found. Querying Gemini without context.")
        context = ""
    else:
        context = "\n\n".join(matched_docs)
        print(f"[RAGServer Tool] Retrieved {len(matched_docs)} documents.")

    # --- Build Prompt for Gemini (server-side) ---
    prompt = f"""You are a helpful assistant. Use the following context to answer the the question. If the context does not contain enough information, state that clearly and then try to answer based on your general knowledge.

Context:
{context}

Question: {user_query}
Answer:"""

    # --- Query Gemini (server-side) ---
    try:
        response = await rag_context.gemini_model.generate_content_async(prompt)
        gemini_answer = response.text.strip()
        print("[RAGServer Tool] Gemini response generated.")
        return gemini_answer
    except Exception as e:
        print(f"[RAGServer Tool] Error querying Gemini: {e}")
        return f"An error occurred while generating the answer: {e}"

# --- Run the FastMCP Server ---
if __name__ == "__main__":
    mcp.run()